{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "048c6368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import datetime\n",
    "\n",
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Import TFIDFVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "#tokenizers\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "\n",
    "#neural network RNN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GRU, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30ed3c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>price</th>\n",
       "      <th>pct_change</th>\n",
       "      <th>price_direction</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2011-07-29</td>\n",
       "      <td>Drug App Comes Free, Ads Included. Epocrates h...</td>\n",
       "      <td>1292.28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>first</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-07-30</td>\n",
       "      <td>Global Concern Over U.S. Debt Ceiling Disagree...</td>\n",
       "      <td>1292.28</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>same</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-07-31</td>\n",
       "      <td>Deal May Avert Default, but Some Ask, ‘Is That...</td>\n",
       "      <td>1292.28</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>same</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-08-01</td>\n",
       "      <td>Charging a Premium for Movies, at a Cost. High...</td>\n",
       "      <td>1286.94</td>\n",
       "      <td>-0.004132</td>\n",
       "      <td>down</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-08-02</td>\n",
       "      <td>Pearls, Finer but Still Cheap, Flow From China...</td>\n",
       "      <td>1254.05</td>\n",
       "      <td>-0.025557</td>\n",
       "      <td>down</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        date                                               text  \\\n",
       "0           0  2011-07-29  Drug App Comes Free, Ads Included. Epocrates h...   \n",
       "1           1  2011-07-30  Global Concern Over U.S. Debt Ceiling Disagree...   \n",
       "2           2  2011-07-31  Deal May Avert Default, but Some Ask, ‘Is That...   \n",
       "3           3  2011-08-01  Charging a Premium for Movies, at a Cost. High...   \n",
       "4           4  2011-08-02  Pearls, Finer but Still Cheap, Flow From China...   \n",
       "\n",
       "     price  pct_change price_direction  day_of_week  is_holiday  \n",
       "0  1292.28         NaN           first            4           0  \n",
       "1  1292.28    0.000000            same            5           0  \n",
       "2  1292.28    0.000000            same            6           0  \n",
       "3  1286.94   -0.004132            down            0           0  \n",
       "4  1254.05   -0.025557            down            1           0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to load file \n",
    "daily = pd.read_csv('/Users/plarkin/Downloads/daily.csv')\n",
    "daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac7256f",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34f2fa",
   "metadata": {},
   "source": [
    "#### Vader Sentiment Analysis\n",
    "\n",
    "#### Textblob Sentiment Analysis\n",
    "source : https://neptune.ai/blog/sentiment-analysis-python-textblob-vs-vader-vs-flair\n",
    "\n",
    "https://textblob.readthedocs.io/en/dev/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbcbe783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.96 µs\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.77 µs\n"
     ]
    }
   ],
   "source": [
    "#Adding in Sentiment analysis with designated columns for each output (pos, neg, neu, compound)\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "#daily['vader'] = daily['text'].map(lambda x:analyzer.polarity_scores(str(x)))\n",
    "\n",
    "\n",
    "daily['vader_compound'] = [analyzer.polarity_scores(x)['compound'] for x in daily['text']]\n",
    "# draft_df['vd_neg'] = [analyzer.polarity_scores(x)['neg'] for x in draft_df['alltext']]\n",
    "# draft_df['vd_neu'] = [analyzer.polarity_scores(x)['neu'] for x in draft_df['alltext']]\n",
    "# draft_df['vd_pos'] = [analyzer.polarity_scores(x)['pos'] for x in draft_df['alltext']]\n",
    "\n",
    "%time\n",
    "\n",
    "from textblob import TextBlob\n",
    "#testimonial = TextBlob()\n",
    "#draft_df['tb_polarity'] = [testimonial.polarity(x)['polarity'] for x in draft_df['alltext']]\n",
    "#draft_df['tb_subj'] = [testimonial.sentiment(x)['subjectivity'] for x in draft_df['alltext']]\n",
    "daily['textblob_polarity'] = daily['text'].map(lambda words: TextBlob(str(words)).polarity) #polarity is more applicable and comparable to vader compound. subjectivity is more about opinion vs fact \n",
    "\n",
    "%time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380b3fea",
   "metadata": {},
   "source": [
    "### Text Cleaning\n",
    "eliminate the punctuation, URL, and @\n",
    "#source: https://monkeylearn.com/blog/text-cleaning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90839bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this to remove http, punctuation, URL, and @\n",
    "daily['text'] = daily['text'].map(lambda x: re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", str(x.lower())))\n",
    "#convert price_direction to numerical and drop first row with NA value\n",
    "daily.dropna(inplace=True)\n",
    "daily['price_direction'] = daily['price_direction'].map({'down' : -1,'same' : 0 , 'up' : 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976ec4c3",
   "metadata": {},
   "source": [
    "tokenize and lemmatize\n",
    "(no longer lemmatizing, results from gridsearch showed superior accuracy without lemmatizing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e580b7",
   "metadata": {},
   "source": [
    "## RNN with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9761a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declaring features and target variable for tfidf. It will not take an array as the X input\n",
    "the_text = daily['text']\n",
    "the_target = daily['price_direction']\n",
    "\n",
    "# TFIDF, increased to 5000 to capture more dates\n",
    "rnn_tvec = TfidfVectorizer(stop_words='english', max_features = 5000, ngram_range= (1,2))\n",
    "rnn_tvec_df = pd.DataFrame(rnn_tvec.fit_transform(the_text).todense(), columns=rnn_tvec.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cb482d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>price</th>\n",
       "      <th>pct_change</th>\n",
       "      <th>price_direction</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>textblob_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-07-30</td>\n",
       "      <td>global concern over us debt ceiling disagreeme...</td>\n",
       "      <td>1292.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.072709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        date                                               text  \\\n",
       "1           1  2011-07-30  global concern over us debt ceiling disagreeme...   \n",
       "\n",
       "     price  pct_change  price_direction  day_of_week  is_holiday  \\\n",
       "1  1292.28         0.0                0            5           0   \n",
       "\n",
       "   vader_compound  textblob_polarity  \n",
       "1           0.664           0.072709  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a1c110c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3652 entries, 0 to 3651\n",
      "Columns: 5000 entries, 10 to zuckerberg mark\n",
      "dtypes: float64(5000)\n",
      "memory usage: 139.3 MB\n"
     ]
    }
   ],
   "source": [
    "rnn_tvec_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d714b608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>10 billion</th>\n",
       "      <th>10 percent</th>\n",
       "      <th>100</th>\n",
       "      <th>100 million</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>12 billion</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>price</th>\n",
       "      <th>pct_change</th>\n",
       "      <th>price_direction</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>textblob_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-07-30</td>\n",
       "      <td>global concern over us debt ceiling disagreeme...</td>\n",
       "      <td>1292.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6640</td>\n",
       "      <td>0.072709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2011-07-31</td>\n",
       "      <td>deal may avert default but some ask is that go...</td>\n",
       "      <td>1292.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.094384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 5010 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10  10 billion  10 percent  100  100 million   11   12  12 billion   13  \\\n",
       "1  0.0         0.0         0.0  0.0          0.0  0.0  0.0         0.0  0.0   \n",
       "2  0.0         0.0         0.0  0.0          0.0  0.0  0.0         0.0  0.0   \n",
       "\n",
       "    14  ...  Unnamed: 0        date  \\\n",
       "1  0.0  ...           1  2011-07-30   \n",
       "2  0.0  ...           2  2011-07-31   \n",
       "\n",
       "                                                text    price  pct_change  \\\n",
       "1  global concern over us debt ceiling disagreeme...  1292.28         0.0   \n",
       "2  deal may avert default but some ask is that go...  1292.28         0.0   \n",
       "\n",
       "   price_direction  day_of_week  is_holiday  vader_compound  textblob_polarity  \n",
       "1                0            5           0          0.6640           0.072709  \n",
       "2                0            6           0          0.9962           0.094384  \n",
       "\n",
       "[2 rows x 5010 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merging vectorized dataframe with original dataset including sentiment analysis. More features!\n",
    "merged_df = rnn_tvec_df.join(daily, how ='inner',lsuffix = '_')\n",
    "merged_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14bf0b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting date as index and dropping text column for modeling\n",
    "merged_df.set_index('date', inplace=True)\n",
    "merged_df.sort_index(inplace=True)\n",
    "\n",
    "merged_df.drop(columns= ['text'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bae4a7a",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c959e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_df.drop(columns= ['price_direction'])\n",
    "y = merged_df[['price_direction']].values\n",
    "yy = merged_df['price_direction']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f844f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    0.380992\n",
       "-1    0.311148\n",
       " 0    0.307861\n",
       "Name: price_direction, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40132c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sticking with a test size of 0.20 to save 2 years of data to test on\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, shuffle=False, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80a8da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to one hot encode multiclass target in order to process in nn\n",
    "y_train = to_categorical(y_train, 3)\n",
    "y_test = to_categorical(y_test, 3)\n",
    "\n",
    "#source https://stackoverflow.com/questions/61550026/valueerror-shapes-none-1-and-none-3-are-incompatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f99a700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale for neural networks\n",
    "ss = StandardScaler()\n",
    "\n",
    "Xs_train = ss.fit_transform(X_train)\n",
    "Xs_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08ba85e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the length parameter dictates how many rows will constitute a sample\n",
    "train_sequences = TimeseriesGenerator(Xs_train, y_train, length=90, batch_size=268) #increased batch sizes from 64 to 268\n",
    "# test sequences\n",
    "test_sequences = TimeseriesGenerator(Xs_test, y_test, length=90, batch_size=268)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "657af9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(268, 90, 5007)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3f453f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 5007)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = train_sequences[0][0][0].shape\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17971ffe",
   "metadata": {},
   "source": [
    "# idea, add some more regularization \n",
    "https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3885b5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modeling and layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(GRU(8, input_shape=input_shape, return_sequences= True))\n",
    "model.add(GRU(8, return_sequences= False))\n",
    "\n",
    "\n",
    "model.add(Dense(32, activation='relu'))#added and increased both hidden layers to 32\n",
    "model.add(BatchNormalization()) #added to help regularize model\n",
    "model.add(Dropout(0.4)) #added to help regularize model\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.4)) #added to help regularize model\n",
    "model.add(Dense(3, activation='softmax')) #softmax for multi-classification\n",
    "\n",
    "#compile it\n",
    "model.compile(optimizer='Adam', loss='CategoricalCrossentropy', metrics=['acc']) #categorical crossentropy for multi-classification\n",
    "\n",
    "#fit it\n",
    "#adding early stopping as a regularization technique\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "history = model.fit(train_sequences, validation_data=test_sequences, epochs=20, verbose=0, callbacks=[early_stop]) #increased epochs from 100 to 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aad430",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot our results\n",
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label='Test loss')\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e6dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'], label='Train accuracy')\n",
    "plt.plot(history.history['val_acc'], label='Test accuracy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b058db88",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f88cef4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'History' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-25891bc9d44a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'History' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "history.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f2eb2c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4836193323135376"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b741da3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1565308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "all_files.append(filename)\n",
    "joblib.dump(history, filename)\n",
    "\n",
    "# Create and train a new model instance.\n",
    "model = create_model()\n",
    "model.fit(train_images, train_labels, epochs=5)\n",
    "\n",
    "# Save the entire model as a SavedModel.\n",
    "!mkdir -p saved_model\n",
    "model.save('saved_model/my_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae36ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = joblib.load(filename)\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27528818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "#all_files= []\n",
    "filename = 'model2_500epochs.sav'\n",
    "all_files.append(filename)\n",
    "\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df82ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312fa0c9",
   "metadata": {},
   "source": [
    "RNN Model history/specs\n",
    "\n",
    "FIRST\n",
    "max test accuracy 0.45553821325302124\n",
    "\n",
    "\n",
    "#modeling and layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(GRU(8, input_shape=input_shape, return_sequences= True))\n",
    "model.add(GRU(8, return_sequences= False))\n",
    "\n",
    "\n",
    "model.add(Dense(2, activation='relu'))\n",
    "model.add(Dense(2, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "#compile it\n",
    "model.compile(optimizer=Adam(learning_rate=.0005), loss='CategoricalCrossentropy', metrics=['acc'])\n",
    "\n",
    "#fit it\n",
    "history = model.fit(train_sequences, validation_data=test_sequences, epochs=100, verbose=0)\n",
    "\n",
    "notes: maxing out after 15-20 epochs \n",
    "\n",
    "SECOND\n",
    "max test accuracy 0.4570982754230499\n",
    "\n",
    "#modeling and layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(GRU(8, input_shape=input_shape, return_sequences= True))\n",
    "model.add(GRU(8, return_sequences= False))\n",
    "\n",
    "\n",
    "model.add(Dense(8, activation='relu'))#added and increased both hidden layers to 8\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "#compile it\n",
    "model.compile(optimizer=Adam(learning_rate=.0005), loss='CategoricalCrossentropy', metrics=['acc'])\n",
    "\n",
    "#fit it\n",
    "history = model.fit(train_sequences, validation_data=test_sequences, epochs=300, verbose=0) #increased epochs from 100 to 300\n",
    "\n",
    "notes: maxing out after 20 epochs \n",
    "\n",
    "THIRD\n",
    "0.4586583375930786\n",
    "\n",
    "#modeling and layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(GRU(8, input_shape=input_shape, return_sequences= True))\n",
    "model.add(GRU(8, return_sequences= False))\n",
    "\n",
    "\n",
    "model.add(Dense(32, activation='relu'))#added and increased both hidden layers to 32\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "#compile it\n",
    "model.compile(optimizer='Adam', loss='CategoricalCrossentropy', metrics=['acc'])\n",
    "\n",
    "#fit it\n",
    "\n",
    "history = model.fit(train_sequences, validation_data=test_sequences, epochs=20, verbose=0) #decreased epochs from 300 to 20\n",
    "\n",
    "\n",
    "FOURTH\n",
    "0.4602183997631073\n",
    "added earlystopping\n",
    "\n",
    "#modeling and layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(GRU(8, input_shape=input_shape, return_sequences= True))\n",
    "model.add(GRU(8, return_sequences= False))\n",
    "\n",
    "\n",
    "model.add(Dense(32, activation='relu'))#added and increased both hidden layers to 32\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax')) #softmax for multi-classification\n",
    "\n",
    "#compile it\n",
    "model.compile(optimizer='Adam', loss='CategoricalCrossentropy', metrics=['acc']) #categorical crossentropy for multi-classification\n",
    "\n",
    "#fit it\n",
    "#adding early stopping as a regularization technique\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "history = model.fit(train_sequences, validation_data=test_sequences, epochs=20, verbose=0, callbacks=[early_stop]) \n",
    "\n",
    "FIFTH\n",
    "0.4492979645729065\n",
    "\n",
    "#modeling and layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(GRU(8, input_shape=input_shape, return_sequences= True))\n",
    "model.add(GRU(8, return_sequences= False))\n",
    "\n",
    "\n",
    "model.add(Dense(32, activation='relu'))#added and increased both hidden layers to 32\n",
    "model.add(BatchNormalization()) #added to help regularize model\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax')) #softmax for multi-classification\n",
    "\n",
    "#compile it\n",
    "model.compile(optimizer='Adam', loss='CategoricalCrossentropy', metrics=['acc']) #categorical crossentropy for multi-classification\n",
    "\n",
    "#fit it\n",
    "#adding early stopping as a regularization technique\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "history = model.fit(train_sequences, validation_data=test_sequences, epochs=20, verbose=0, callbacks=[early_stop]) #increased epochs from 100 to 300\n",
    "\n",
    "\n",
    "SIXTH\n",
    "0.4836193323135376\n",
    "adding dropout 0.2\n",
    "\n",
    "#modeling and layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(GRU(8, input_shape=input_shape, return_sequences= True))\n",
    "model.add(GRU(8, return_sequences= False))\n",
    "\n",
    "\n",
    "model.add(Dense(32, activation='relu'))#added and increased both hidden layers to 32\n",
    "model.add(BatchNormalization()) #added to help regularize model\n",
    "model.add(Dropout(0.2)) #added to help regularize model\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2)) #added to help regularize model\n",
    "model.add(Dense(3, activation='softmax')) #softmax for multi-classification\n",
    "\n",
    "#compile it\n",
    "model.compile(optimizer='Adam', loss='CategoricalCrossentropy', metrics=['acc']) #categorical crossentropy for multi-classification\n",
    "\n",
    "#fit it\n",
    "#adding early stopping as a regularization technique\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "history = model.fit(train_sequences, validation_data=test_sequences, epochs=20, verbose=0, callbacks=[early_stop]) #increased epochs from 100 to 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc07846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52974c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1489f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, preds, sample_weight=None)\n",
    "#aim for best accuracy, but also reduce false positives and negatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "800396b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'value_counts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-03c9761490d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# calculate null accuracy (for multi-class classification problems)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# .head(1) assesses the value 1208\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnull_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Null accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnull_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'value_counts'"
     ]
    }
   ],
   "source": [
    "# calculate null accuracy (for multi-class classification problems)\n",
    "# .head(1) assesses the value 1208\n",
    "null_accuracy = y_test.value_counts().head(1) / len(y_test)\n",
    "print('Null accuracy:', null_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16224e1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-765ae4ba0d7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# print message text for the false negatives (spam incorrectly classified as ham)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vader_compound'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# alternative less elegant but easier to understand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "# print message text for the false negatives (spam incorrectly classified as ham)\n",
    "\n",
    "fn= pd.DataFrame(X_test[preds < y_test])\n",
    "fn['vader_compound']\n",
    "# alternative less elegant but easier to understand\n",
    "# X_test[(y_pred_class=0) & (y_test=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d705a",
   "metadata": {},
   "source": [
    "# TODO\" model best params with multinomial bayes, how accurate is it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e443ae9",
   "metadata": {},
   "source": [
    "## Evaluation of Models\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
