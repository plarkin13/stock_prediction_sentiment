{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d4380b",
   "metadata": {},
   "source": [
    "# Predicting the S&P 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fc0200",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35cfa86",
   "metadata": {},
   "source": [
    "- Background\n",
    "- Approach Overview:\n",
    "- The Data\n",
    "- EDA and Modeling\n",
    "- Classification and Modeling using no text data\n",
    "- Regression Modeling using text data\n",
    "- Classification and Regression Modeling using no text data\n",
    "- Results\n",
    "- Conclusions\n",
    "- Future Recs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44823711",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "This analysis is being performed by three friends that want to supplement their incomes as data scientists in the S&P 500. With mad data wizardry skills,  the three scientists will evaluate if using news from the New York Times (NYT) can increase their ability to predict market movement in the S&P 500, giving them a trading advantage. \n",
    "\n",
    "We came to this question when trying to tackle this problem. Is it possible that sentiment analysis on business articles coupled with other natural language processing (NLP) techniques predict S&P 500 price data with greater than 50% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e2857f",
   "metadata": {},
   "source": [
    "### Approach Overview:\n",
    "- Used NYT API and S&P 500 to gather 10 years of data\n",
    "- Cleaned and filtered data to key components\n",
    "- Performed extensive EDA on both S&P 500 and NYT text data\n",
    "- Parallelization of regression & classification modeling\n",
    "- Compare top performing models\n",
    "\n",
    "Software requirements: Pandas, Scikit-learn, Tenserflow, Numpy, and Matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b0cf95",
   "metadata": {},
   "source": [
    "## The Data\n",
    "Data sources:\n",
    "10 years of New York Times articles (2011-2021)\n",
    "10 years of S&P 500 data from Yahoo Finance\n",
    "\n",
    "In order to acquire NYT data used an API (source: \n",
    "Forked and customized a version of pynytimes in order to access the API (source: https://github.com/michadenheijer/pynytimes). Ten articles per day for 10 years (2011-2021) were gathered from the business section of the NYT.\n",
    "\n",
    "To gather relevant S&P 500 data:\n",
    "Used Yahoo Finance (source: https://pypi.org/project/fix-yahoo-finance/0.1.30/).\n",
    "\n",
    "Feature Selection:\n",
    "- Identified relevant data from API (text and date)\n",
    "- Merged all text features (heading, lead paragraph, and abstract)\n",
    "\n",
    "Feature Engineering:\n",
    "- Percent change in price\n",
    "- Direction of change\n",
    "- Added day of the week and holidays\n",
    "\n",
    "Feature Cleaning:\n",
    "- Removing text issues of http, punctuation, URL, and @\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01521eb",
   "metadata": {},
   "source": [
    "## EDA and Modeling\n",
    "In order to ascertain the strongest relationships between data and price prediction the group decided to distribute the modeling tasks into three buckets: classification with text data, regression with text data, and both classification and regression modeling using no text data.\n",
    "\n",
    "The goal of the multi-classification problem was to predict if price the following day will go down, up, or stay the same. Accuracy will be the metric used to verify models and compare them. The baseline accuracy score: 38.01%.\n",
    "\n",
    "Since accuracy is a measure for classification, the goal of the regression models was to reduce MSE and MAE to establish a basic relationship between text sentiment and the movement of the S&P 500. The error scores were produced using a GridSearch, producing best parameters for measurement of the relationship.\n",
    "\n",
    "**Mean square error (MSE)** is the average of the square of the errors. The larger the number the larger the error. MSE is an error metric, i.e. the lower the better.\n",
    "\n",
    "**Mean Absolute Error (MAE)**. MAE is simply, as the name suggests, the mean of the absolute errors. The absolute error is the absolute value of the difference between the forecasted value and the actual value. MAE tells us how big of an error we can expect from the forecast on average. MAE is an error metric, i.e. the lower the better.\n",
    "\n",
    "https://stackoverflow.com/questions/45627784/unable-to-obtain-accuracy-score-for-my-linear\n",
    "\n",
    "Collaborated test size of 20%, 2 years of data for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc827c3e",
   "metadata": {},
   "source": [
    "### Classification and Modeling using text data\n",
    "In order to properly analyze the text multiple approaches were used. Two different types of sentiment analysis were incorporated, Vader and TextBlob. Ultimately both were incorporated to enhance the capture of sentiment. Two different types of vectorizing was used, CountVectorizing (CV) and TFIDF. The TFIDF performed the best from the multiple models tested and was the designated tokenizer for the rest of the modeling.\n",
    "\n",
    "There was also considerable experimentation with lemmatizing text to reduce features and consolidate the weights of words. The models with lemmatizing performed worse than models without lemmatizing. So the next step was to identify if there were further features in the text to capture so the text was tokenized. Within the tokenizing search n_grams were evaluated from 1-3 words and the best n_grams were Many variations of models were run for these different iterations of text extraction through grid searching, and in the end sentiment analyis without any other text features performed best.\n",
    "\n",
    "Within the tested neural networks RNN was used as a preliminary model, but had problems with overfitting. LSTM provided higher accuracy scores with less overfitting.\n",
    "\n",
    "**Models: (Accuracy Score)**\n",
    "*Lemmatize vs non-lemmatize*\n",
    "TFIDF w/ Multinomial Bayes: 52.4%\n",
    "CV w/ Multinomial Bayes: 44.2%\n",
    "\n",
    "*Tokenize text and sentiment analysis*\n",
    "TFIDF w/ Multinomial Bayes: 55.14%\n",
    "TFIDF w/ RNN: 48.36%\n",
    "\n",
    "*Only sentiment analysis*\n",
    "RNN: 64.57%\n",
    "LSTM: 77.02%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846c91dd",
   "metadata": {},
   "source": [
    "###  Regression Modeling using text data\n",
    "\n",
    "In order to properly analyze the text, multiple approaches were used. Two different types of sentiment analysis were incorporated, Vader and TextBlob. Ultimately, the Vader sentiment analysis was more correlative to the direction of the price so it was used in concert with the movement in the market measured in percentage.\n",
    "\n",
    "Text was left as raw as possible for the regression models to create a baseline of understanding to see if further and more substantial analysis was merited. This means that the data used for the regression models were not lemmatized, nor were key words and stop words utilized to alter the outcome.\n",
    "\n",
    "Overall, 8 different regression models were tested using a basic pipeline.\n",
    "- Linear Regression\n",
    "- Gradient Boost\n",
    "- KNeighbors\n",
    "- Ridge\n",
    "- Extra Trees\n",
    "- Random Forest\n",
    "- Lasso\n",
    "- Decision Tree\n",
    "\n",
    "Based on R2 scores, the top three performing models were selected and put through a gridsearch to optimize their potential.\n",
    "- Random Forest - 0.32MSE, 0.10MAE\n",
    "- Extra Trees - 0.23MSE, 0.36MAE\n",
    "- Decision Tree - 0.32MSE, 0.45MAE\n",
    "\n",
    "Consistent with the other modeling techniques used, the Random Forest regressor performed well enough to justify further modeling and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea50acc",
   "metadata": {},
   "source": [
    "### Classification and Regression Modeling using no text data\n",
    "\n",
    "**Classification Models: (Accuracy Score)**\n",
    "Multinomial Bayes: 0.55\n",
    "FFNN: 0.80\n",
    "Recurrent Neural Network (RNN): 0.65\n",
    "LSTM: 0.81 \n",
    "Logistic Regression: 0.44 \n",
    "KNN: 0.77\n",
    "Decision Tree: 0.78 \n",
    "Bagging Classifier: 0.79\n",
    "Random Forest: 0.80 \n",
    "Gradient Boosting: 0.81\n",
    "XGB Classifier: 0.80\n",
    "SVC: 0.80\n",
    "\n",
    "**Regression Models: (MSE)**\n",
    "Gradient Boosting: 6.30E-05\n",
    "Random Forest Regressor: 6.73E-05\n",
    "Bagging Regressor: 6.52E-05\n",
    "LSTM: 7.61E-05\n",
    "FFNN: 7.69E-05\n",
    "Decision Tree Regressor: 8.06E-05\n",
    "Linear Regression: 8.13E-05\n",
    "RNN: 1.20E-04\n",
    "SVR: 2.19E-04\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560d1150",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Random Forest Regressor using sentiment\n",
    "\n",
    "- Best Parameters: \n",
    "\n",
    "Criterion: **MSE** Max Features: **auto**\n",
    "***0.32MSE, 0.10MAE**\n",
    "\n",
    "**Random Forest Classifier**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Importance\n",
    "- Open_pct_l1:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.031\n",
    "- close_pct_l1:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.479\n",
    "- Volume_diff_l1:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.223\n",
    "- is_holiday_1:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.024\n",
    "- day_of_week_1:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.004\n",
    "- day_of_week_2:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.004\n",
    "- day_of_week_3:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.007\n",
    "- day_of_week_4:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.009\n",
    "- day_of_week_5:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.005\n",
    "- day_of_week_6:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.215\n",
    "\n",
    "\n",
    "<br><br>**Gradient Boosting Regressor**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Importance\n",
    "- Open_pct_l1:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.017\n",
    "- close_pct_l1:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.973\n",
    "- Volume_diff_l1:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.008\n",
    "- is_holiday_1:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.000\n",
    "- day_of_week_1:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.001\n",
    "- day_of_week_2:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.000\n",
    "- day_of_week_3:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.001\n",
    "- day_of_week_4:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.000\n",
    "- day_of_week_5:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.000\n",
    "- day_of_week_6:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e25ea5",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Can sentiment analysis on business articles coupled with other NLP techniques predict S&P 500 price data  with greater than 50% accuracy?\n",
    "YES! The models incorporating tokenized text (55.13%) and only sentiment (77.01%) beat that 50% bar. But the best models in prediction did not incorporate the news. The bar of 50% appears to be low after identifying ways to identify pricing trends using purely data from previous days. One possible theory for the model to not perform as well with sentiment of text is that it created more noise than aid. The correlating factors between sentiment of news and price direction were not particularly strong.\n",
    "\n",
    "**Best Regression Model using Sentiment Analysis** Random Forest Regressor - 0.105MAE<br>\n",
    "This Regression analysis was a stepping stone to determine vaibility of concept and to stand as a baseline for further effort. With a 10.5% mean average error, it more than exeeded expectations and further analysis is warranted.\n",
    "\n",
    "**Best Models WITHOUT using news data**\n",
    "Random Forest Classifier - 80% Accurate\n",
    "Gradient Boosting Regressor - 6.03E-05\n",
    "\n",
    "Focusing on the percent change of closing/opening prices along with volume difference is a better use of your forecasting power in predicting market movement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f258fec",
   "metadata": {},
   "source": [
    "### Future Recs\n",
    "- Try different news sources (Bloomberg/Wall Street Journal)\n",
    "- Try multiple news sources \n",
    "- Try social media posts to gauge market sentiment instead\n",
    "- Try more advanced word vectorizers \n",
    "- Incorporate other features about the market such as the moving average, dividends, earnings reports etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ca6df0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
